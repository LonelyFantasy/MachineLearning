{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【实验1】一元线性回归预测电影的票房收入\n",
    "### 电影投资与收入信息\n",
    "![电影投资与收入信息](regression-1.png)\n",
    "\n",
    "## 接下来要拍一部投资2千万的电影，使用一元线性回归预测一下新电影的票房收入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤一：使用数据绘制图，发现规律\n",
    "import matplotlib.pyplot as plt\n",
    "def drawplt():\n",
    "    plt.figure()\n",
    "    plt.title('Cost and Income Of a Film')\n",
    "    plt.xlabel('Cost(Million Yuan)')\n",
    "    plt.ylabel('Income(Million Yuan)')\n",
    "    plt.axis([0, 25, 0, 60])\n",
    "    plt.grid(True)\n",
    "    \n",
    "X = [[6], [9], [12], [14], [16]]\n",
    "y = [[9], [12], [29], [35], [59]]\n",
    "drawplt()\n",
    "plt.plot(X, y, 'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#步骤二：线性回归预测电影票房收入\n",
    "from sklearn import linear_model\n",
    "def drawplt():\n",
    "    plt.figure()\n",
    "    plt.title('Cost and Income Of a Film')\n",
    "    plt.xlabel('Cost(Million Yuan)')\n",
    "    plt.ylabel('Income(Million Yuan)')\n",
    "    plt.axis([0, 25, 0, 60])\n",
    "    plt.grid(True)\n",
    "\n",
    "X = [[6], [9], [12], [14], [16]]\n",
    "y = [[9], [12], [29], [35], [59]]\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X, y)\n",
    "# a = model.predict([[20]])\n",
    "print(\"投资2千万的电影预计票房收入为：{:.2f}百万元\".format(model.predict([[20]])[0][0]))\n",
    "print(\"回归模型的系数是：\",model.coef_ )\n",
    "print(\"回归模型的截距是：\",model.intercept_)  \n",
    "print(\"最佳拟合线: y = \",int(model.intercept_),\"＋\", int(model.coef_),\"× x\")\n",
    "drawplt()\n",
    "print(X)\n",
    "plt.plot(X, y, 'k.')\n",
    "plt.plot([0,25],[model.intercept_,25*model.coef_+model.intercept_])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【实验2】多元线性回归预测电影票房\n",
    "### 在上面的数据基础上，又搜集到了每部电影的广告费用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "x = np.array([[6,1,9],[9,3,12],[12,2,29],\n",
    "              [14,3,35],[16,4,59]])\n",
    "X = x[:,:-1]\n",
    "Y = x[:,-1]\n",
    "print('X:',X)\n",
    "print('Y:',Y)\n",
    "\n",
    "# 训练数据\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X,Y)\n",
    "print('系数(w1,w2)为:',regr.coef_)\n",
    "print('截距(b)为:',regr.intercept_)\n",
    "\n",
    "# 预测\n",
    "y_predict = regr.predict(np.array([[10,3]]))\n",
    "print('投资1千万，推广3百万的电影票房预测为：',y_predict,'百万')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用sk-learn进行波士顿房价预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error #平方绝对误差\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "# print(boston.DESCR)   #获得关于房价的描述信息\n",
    "x = boston.data       #获得数据集的特征属性列\n",
    "y = boston.target     #获得数据集的label列\n",
    "df = pd.DataFrame(data = np.c_[x,y],columns=np.append(boston.feature_names,['MEDV'])) \n",
    "#np.c_是按列连接两个矩阵，就是把两矩阵左右相加，要求列数相等\n",
    "\n",
    "df = df[['RM','MEDV']]      #选择房间数属性列、房价属性列（预测目标）\n",
    "print(df[:5])         #查看前5行的数据格式\n",
    "\n",
    "x1=df[['RM']] \n",
    "y1=df[['MEDV']] \n",
    "x_train,x_test,y_train,y_test = train_test_split(x1,y1,test_size=0.4)     #划分数据集\n",
    "\n",
    "# x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.4)     #划分数据集\n",
    "\n",
    "scaler = StandardScaler()   #作用：去均值和方差归一化。可保存训练集中的均值、方差参数，然后直接用于转换测试集数据。\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "model = linreg.fit(x_train,y_train)\n",
    "\n",
    "print('系数(w)为:',model.coef_)\n",
    "print('截距(b)为:',model.intercept_)\n",
    "\n",
    "a = model.predict([[12]])\n",
    "print('预计房价为：',a,'千美元')\n",
    "\n",
    "print(\"\\nMSE均方误差：\",mean_squared_error(y_train,model.predict(x_train)))\n",
    "print(\"RMSE均方根误差：\",mean_squared_error(y_train,model.predict(x_train)) ** 0.5)\n",
    "print(\"MAE平均绝对误差：\",mean_absolute_error(y_train,model.predict(x_train)))\n",
    "print(\"r2_score决定系数：\",r2_score(y_train,model.predict(x_train)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例：使用逻辑回归（对数几率回归）进行贷款逾期情况预测\n",
    "## 方法一、使用sklearn的LogisticRegression模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "data=pd.read_csv('credit-overdue.csv') #读文件\n",
    "#print(data)\n",
    "X=data.iloc[:,0:2]\n",
    "y=data.iloc[:,2]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train, y_train)\n",
    "\n",
    "print('coef:\\n',clf.coef_)\n",
    "print('intercept:\\n',clf.intercept_ )\n",
    "\n",
    "print('predict first two:\\n',clf.predict(X_train.iloc[:,0:2]))\n",
    "print('classification score:\\n',clf.score(X_train, y_train))\n",
    "\n",
    "predict_y = clf.predict(X_test)\n",
    "print('classfication report:\\n ',metrics.classification_report(y_test,predict_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法二  使用Python实现逻辑回归\n",
    "### 先自定义Sigmoid 函数、损失函数\n",
    "### 然后进行梯度下降，经过迭代，确定模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) 步骤一. 加载数据集\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"credit-overdue.csv\", header=0) # 加载数据集\n",
    "df.head()  #查看前5行数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#（2）步骤二.绘制数据的散点图，查看数据分布情况\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['debt'],df['income'], c=df['overdue'])\n",
    "plt.title('Credit-overdue')\n",
    "plt.xlabel('Debt')\n",
    "plt.ylabel('Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#（3）步骤三. 定义Sigmoid 函数、损失函数，使用梯度下降确定模型参数\n",
    "\n",
    "#定义Sigmoid函数\n",
    "def sigmoid(z):\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    return sigmoid\n",
    "\n",
    "#定义对数损失函数\n",
    "def loss(h, y):\n",
    "    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    return loss\n",
    "\n",
    "#定义梯度下降函数\n",
    "def gradient(X, h, y):\n",
    "    gradient = np.dot(X.T, (h - y)) / y.shape[0]\n",
    "    return gradient\n",
    "\n",
    "# 逻辑回归过程\n",
    "def Logistic_Regression(x, y, lr, num_iter):\n",
    "    intercept = np.ones((x.shape[0], 1))  # 初始化截距为 1\n",
    "    x = np.concatenate((intercept, x), axis=1)  # axis=1表示对应行的数组进行拼接\n",
    "    w = np.zeros(x.shape[1])  # 初始化参数为 0\n",
    "\n",
    "    for i in range(num_iter):  # 梯度下降迭代\n",
    "        z = np.dot(x, w)  # 线性函数\n",
    "        h = sigmoid(z)  # sigmoid 函数\n",
    "        g = gradient(x, h, y)  # 计算梯度\n",
    "        w -= lr * g  # 通过学习率 lr 计算步长并执行梯度下降\n",
    "        z = np.dot(x, w)  # 更新参数到原线性函数中\n",
    "        h = sigmoid(z)  # 计算 sigmoid 函数值\n",
    "        l = loss(h, y)  # 计算损失函数值\n",
    "    return l, w  # 返回迭代后的损失值和参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#（4）步骤四. 初始化模型，并对模型进行训练\n",
    "#初始化参数\n",
    "import numpy as np\n",
    "x = df[['debt','income']].values\n",
    "y = df['overdue'].values\n",
    "lr = 0.001 # 学习率\n",
    "num_iter = 10000 # 迭代次数\n",
    "# 模型训练\n",
    "L = Logistic_Regression(x, y, lr, num_iter)  # 返回损失值和参数\n",
    "L  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#（5）步骤五.根据得到的参数，绘制模型分类线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['debt'],df['income'], c=df['overdue'])\n",
    "\n",
    "x1_min, x1_max = df['debt'].min(), df['debt'].max(),\n",
    "x2_min, x2_max = df['income'].min(), df['income'].max(),\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "probs = (np.dot(grid, np.array([L[1][1:3]]).T) + L[1][0]).reshape(xx1.shape)\n",
    "plt.contour(xx1, xx2, probs, levels=[0], linewidths=1, colors='red');\n",
    "plt.title('Credit-overdue')\n",
    "plt.xlabel('Debt')\n",
    "plt.ylabel('Income')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"绘制损失函数变化曲线\n",
    "\"\"\"\n",
    "def Logistic_Regression(x, y, lr, num_iter):\n",
    "    intercept = np.ones((x.shape[0], 1))  # 初始化截距为 1\n",
    "    x = np.concatenate((intercept, x), axis=1)\n",
    "    w = np.zeros(x.shape[1])  # 初始化参数为 1\n",
    "\n",
    "    l_list = []  # 保存损失函数值\n",
    "    for i in range(num_iter):  # 梯度下降迭代\n",
    "        z = np.dot(x, w)  # 线性函数\n",
    "        h = sigmoid(z)  # sigmoid 函数\n",
    "\n",
    "        g = gradient(x, h, y)  # 计算梯度\n",
    "        w -= lr * g  # 通过学习率 lr 计算步长并执行梯度下降\n",
    "\n",
    "        z = np.dot(x, w)  # 更新参数到原线性函数中\n",
    "        h = sigmoid(z)  # 计算 sigmoid 函数值\n",
    "\n",
    "        l = loss(h, y)  # 计算损失函数值\n",
    "        l_list.append(l)\n",
    "\n",
    "    return l_list\n",
    "\n",
    "\n",
    "lr = 0.01  # 学习率\n",
    "num_iter = 30000  # 迭代次数\n",
    "l_y = Logistic_Regression(x, y, lr, num_iter)  # 训练\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([i for i in range(len(l_y))], l_y)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss function\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绘制损失函数变化曲线:\n",
    "\n",
    "    可以看到，损失函数的值随着迭代次数的增加而逐渐降低。前面降低地非常快速，达到一定程度后趋于稳定。上面的程序步骤迭代到20000次左右，之后的数据变化比较缓慢，此时就接近于损失函数的极小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【MachineLearning】之 逻辑回归（Python 实现）  2020.2.25\n",
    "\n",
    "https://blog.csdn.net/fanfan4569/article/details/81748980  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【实验3】使用线性判别分析(LDA)实现分类\n",
    "# 实例1： 使用Python实现LDA进行二分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用代码实现 LDA\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X=np.loadtxt(\"22.txt\")  \n",
    "\n",
    "pos0=np.where(X[:,2]==0)  # 0类样本所在行\n",
    "print(pos0)\n",
    "\n",
    "pos1=np.where(X[:,2]==1)  # 1类样本所在行\n",
    "print(pos1)\n",
    "\n",
    "X1=X[pos0,0:2]    # 取出0类样本\n",
    "X1=X1[0,:,:]     \n",
    "print(X1,X1.shape)\n",
    "\n",
    "X2=X[pos1,0:2]     # 取出1类样本\n",
    "X2=X2[0,:,:]\n",
    "print(X2,X2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一步，求各个类别的均值\n",
    "M1=np.mean(X1,0)\n",
    "M1=np.array([M1])\n",
    "print(M1,M1.shape)\n",
    "\n",
    "M2=np.mean(X2,0)\n",
    "M2=np.array([M2])\n",
    "print(M2,M2.shape)\n",
    "\n",
    "M=np.mean(X[:,0:2],0)\n",
    "M=np.array([M])\n",
    "print(M)\n",
    "\n",
    "p=np.size(X1,0)\n",
    "print(p)\n",
    "\n",
    "q=np.size(X2,0)\n",
    "print(q)\n",
    "\n",
    "#第二步，求类内散度矩阵\n",
    "S1=np.dot((X1-M1).transpose(),(X1-M1))\n",
    "print(S1)\n",
    "S2=np.dot((X2-M2).transpose(),(X2-M2))\n",
    "print(S2)\n",
    "Sw=(p*S1+q*S2)/(p+q)\n",
    "print(Sw)\n",
    "\n",
    "#第三步，求类间散度矩阵\n",
    "Sb1=np.dot((M1-M).transpose(),(M1-M))\n",
    "print(Sb1)\n",
    "Sb2=np.dot((M2-M).transpose(),(M2-M))\n",
    "print(Sb2)\n",
    "Sb=(p*Sb1+q*Sb2)/(p+q)\n",
    "print(Sb)\n",
    "\n",
    "#判断Sw是否可逆\n",
    "bb=np.linalg.det(Sw)\n",
    "print(\"\\n判断Sw是否可逆:\")\n",
    "print(bb)\n",
    "\n",
    "#第四步，求最大特征值和特征向量\n",
    "[V,L]=np.linalg.eig(np.dot(np.linalg.inv(Sw),Sb))\n",
    "print(V,L.shape)\n",
    "list1=[]\n",
    "a=V\n",
    "list1.extend(a)\n",
    "print(list1)\n",
    "\n",
    "b=list1.index(max(list1))\n",
    "print(a[b])\n",
    "W=L[:,b]\n",
    "\n",
    "print(W,W.shape)\n",
    "\n",
    "#根据求得的投影向量W画出投影线\n",
    "k=W[1]/W[0]\n",
    "b=0;\n",
    "x=np.arange(2,10)\n",
    "yy=k*x+b\n",
    "\n",
    "plt.plot(x,yy)\n",
    "plt.scatter(X1[:,0],X1[:,1],marker='+',color='r',s=20)\n",
    "plt.scatter(X2[:,0],X2[:,1],marker='*',color='b',s=20)\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#计算第一类样本在直线上的投影点\n",
    "xi=[]\n",
    "yi=[]\n",
    "for i in range(0,p):\n",
    "    y0=X1[i,1]\n",
    "    x0=X1[i,0]\n",
    "    x1=(k*(y0-b)+x0)/(k**2+1)\n",
    "    y1=k*x1+b\n",
    "    xi.append(x1)\n",
    "    yi.append(y1)\n",
    "print(xi)\n",
    "\n",
    "print(yi)\n",
    "\n",
    "#计算第二类样本在直线上的投影点\n",
    "xj=[]\n",
    "yj=[]\n",
    "for i in range(0,q):\n",
    "    y0=X2[i,1]\n",
    "    x0=X2[i,0]\n",
    "    x1=(k*(y0-b)+x0)/(k**2+1)\n",
    "    y1=k*x1+b\n",
    "    xj.append(x1)\n",
    "    yj.append(y1)\n",
    "print(xj)\n",
    "print(yj)\n",
    "\n",
    "#画出投影后的点\n",
    "plt.plot(x,yy)\n",
    "plt.scatter(X1[:,0],X1[:,1],marker='+',color='r',s=20)\n",
    "plt.scatter(X2[:,0],X2[:,1],marker='>',color='b',s=20)\n",
    "plt.grid()\n",
    "plt.plot(xi,yi,'r+')\n",
    "plt.plot(xj,yj,'b>')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例2：使用sklearn 实现LDA 进行鸢尾花分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas 读入鸢尾花数据集\n",
    "import pandas as pd\n",
    "iris_data = pd.read_csv('data/iris.data')\n",
    "\n",
    "#由于这个数据没有列名， 所以先给每个列取个名字。\n",
    "iris_data.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "print(iris_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 第一步 数据载入\n",
    "#data = pd.io.parsers.read_csv(filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "data = pd.io.parsers.read_csv(filepath_or_buffer='data/iris.data',header=None, names=['sepal length in cm', 'sepal width in cm','petal length in cm', 'petal width in cm', 'names'],\n",
    "                             sep=',',)\n",
    "data.dropna(how='all', inplace=True)   # 丢弃缺失数据\n",
    "\n",
    "data.head()  #查看前5行数据\n",
    "\n",
    "# 第二步 提取数据的X轴和y轴信息\n",
    "feature_names = ['sepal length in cm', 'sepal width in cm','petal length in cm', 'petal width in cm']\n",
    "X = data[feature_names].values\n",
    "y = data['names'].values\n",
    "\n",
    "# 第三步 使用Label_encoding进行标签的数字转换\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "model = LabelEncoder().fit(y)\n",
    "y = model.transform(y) + 1\n",
    "labels_type = np.unique(y)\n",
    "\n",
    "# 第四步 建立LDA模型\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "model = LDA(n_components=2)     # n_components=2 ：进行LDA降维时,要降到的维数\n",
    "sklearn_x = model.fit_transform(X, y)\n",
    "\n",
    "# 第五步进行画图操作\n",
    "labels_dict = data['names'].unique()\n",
    "\n",
    "def plot_lda_sklearn(X, title):\n",
    "    ax = plt.subplot(111)\n",
    "    for label, m, c in zip(labels_type, ['*', '+', 'v'], ['red', 'black', 'green']):\n",
    "        plt.scatter(X[y==label][:, 0], X[y==label][:, 1], c=c, marker=m, alpha=0.6, s=100, label=labels_dict[label-1])\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.6)\n",
    "\n",
    "    plt.tick_params(axis='all', which='all', bottom='off', left='off', right='off', top='off',labelbottom='on', labelleft='on')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_lda_sklearn(sklearn_x, 'The Sklearn LDA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【实验4】 使用imbalanced-learn 解决样本不均衡问题\n",
    "##  欠采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({1: 900, 0: 100})\n",
      "[[ 23   0]\n",
      " [  2 225]]\n"
     ]
    }
   ],
   "source": [
    "#  欠采样-1  使用EasyEnsembleClassifier类\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.ensemble import EasyEnsembleClassifier   # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "X, y = make_classification(n_classes=2, class_sep=2,weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "print('Original dataset shape: %s' % Counter(y))\n",
    "# Original dataset shape: Counter({1: 900, 0: 100})   # 1类的样本个数900，0类的样本个数100\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)   # 默认划分比例是：训练集占75%\n",
    "eec = EasyEnsembleClassifier(random_state=42)\n",
    "eec.fit(X_train, y_train)   # doctest: +ELLIPSIS\n",
    "\n",
    "# EasyEnsembleClassifier(...)\n",
    "y_pred = eec.predict(X_test)\n",
    "\n",
    "# confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 4674, 1: 262, 0: 64})\n",
      "[(0, 64), (1, 64), (2, 64)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n当数据量足够（一万或者十万条记录或更多）时使用欠采样，它记录通过减少丰富类的大小来平衡数据集。\\n最简单的方法是通过保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本。\\n（通过设置replacement=True参数可以实现不放回抽样）\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  欠采样-2  使用RandomUnderSampler类\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_repeated=0, n_classes=3,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[0.01, 0.05, 0.94],\n",
    "                           class_sep=0.8, random_state=0)\n",
    "print(Counter(y))    \n",
    " # 标号2类的样本个数：4674，标号1类的样本个数：262，标号0类的样本个数：64\n",
    "    \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_sample(X, y)\n",
    "\n",
    "print(sorted(Counter(y_resampled).items()))\n",
    "\n",
    "\"\"\"\n",
    "当数据量足够（一万或者十万条记录或更多）时使用欠采样，它记录通过减少丰富类的大小来平衡数据集。\n",
    "最简单的方法是通过保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本。\n",
    "（通过设置replacement=True参数可以实现不放回抽样）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.ensemble import EasyEnsemble   imblearn 0.4   已过时！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  过采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({1: 90, 0: 10})\n",
      "Resampled dataset shape Counter({1: 90, 0: 90})\n"
     ]
    }
   ],
   "source": [
    "# SMOTE  过采样-1 \n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "                           weights=[0.1, 0.9], n_informative=2, n_redundant=0, flip_y=0,\n",
    "                           n_features=2, n_clusters_per_class=1, n_samples=100,random_state=10)\n",
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))\n",
    "\n",
    "# https://blog.csdn.net/u010654299/article/details/103980964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  过采样-2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=100, random_state=10)\n",
    "\n",
    "# print(X)\n",
    "# print(X.shape)\n",
    "\n",
    "print('Original dataset shape: %s' % Counter(y))\n",
    "# print(y.shape)\n",
    "print(y)\n",
    "\n",
    "sm = SMOTEENN()\n",
    "X_resampled, y_resampled = sm.fit_sample(X, y)\n",
    "\n",
    "# print(y_resampled.shape)\n",
    "print('\\n Resampled dataset shape: %s' % Counter(y_resampled))\n",
    "print(y_resampled)\n",
    "\n",
    "# https://blog.csdn.net/liweibin1994/article/details/78380841?depth_1-utm_source=distribute.\n",
    "# pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  过/欠采样-3   待完善！ 2020.3.13\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE                # 过抽样处理库SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler  # 欠抽样处理库RandomUnderSample\n",
    "from sklearn.svm import SVC                             # SVM中的分类算法SVC\n",
    "#from imblearn.ensemble import EasyEnsemble             # 简单集成方法EasyEnsemble     old---    ( imblearn 0.4) \n",
    "from imblearn.ensemble import EasyEnsembleClassifier   # 简单集成方法EasyEnsemble   new---  ( imblearn 0.6)\n",
    "\n",
    "# 导入数据文件\n",
    "df = pd.read_csv('data2.txt', sep=' ', names=['col1', 'col2','col3', 'col4', 'col5', 'label'])\n",
    "x = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "groupby_data_orgianl = df.groupby('label').count()      # 对label做分类汇总\n",
    "print(groupby_data_orgianl)                             # 输出原始数据集样本分类分布\n",
    "\n",
    "\n",
    "# SMOTE方法进行过采样处理\n",
    "model_smote = SMOTE()           # 建立SMOTE模型对象\n",
    "x_smote_resampled, y_smote_resampled = model_smote.fit_sample(x, y)         # 输入数据做过抽样处理\n",
    "x_smote_resampled = pd.DataFrame(x_smote_resampled, columns=['col1','col2', 'col3', 'col4', 'col5'])        # 将数据转换为数据框并命名列名\n",
    "y_smote_resampled = pd.DataFrame(y_smote_resampled,columns=['label'])       # 将数据转换为数据框并命名列名\n",
    "smote_resampled = pd.concat([x_smote_resampled, y_smote_resampled],axis=1)  # 按列合并数据框\n",
    "groupby_data_smote = smote_resampled.groupby('label').count()               # 对label做分类汇总\n",
    "print (groupby_data_smote)      # 打印输出经过SMOTE处理后的数据集样本分类分布\n",
    "\n",
    "\n",
    "# RandomUnderSampler方法进行欠抽样处理\n",
    "model_RandomUnderSample = RandomUnderSampler()          # 建立RandomUnderSampler模型对象\n",
    "x_RandomUnderSampler_resampled, y_RandomUnderSampler_resampled = model_RandomUnderSample.fit_sample(x, y)   # 输入数据做欠抽样处理\n",
    "x_RandomUnderSampler_resampled = pd.DataFrame(x_RandomUnderSampler_resampled, columns=['col1','col2','col3','col4','col5'])\n",
    "y_RandomUnderSampler_resampled =pd.DataFrame(y_RandomUnderSampler_resampled,columns=['label'])\n",
    "RandomUnderSampler_resampled =pd.concat([x_RandomUnderSampler_resampled, y_RandomUnderSampler_resampled], axis= 1)  # 按列合并数据框\n",
    "groupby_data_RandomUnderSampler =RandomUnderSampler_resampled.groupby('label').count()                              # 对label做分类汇总\n",
    "print (groupby_data_RandomUnderSampler)                 # 打印输出经过RandomUnderSampler处理后的数据集样本分类分布\n",
    "\n",
    "\n",
    "# 使用SVM的权重调节处理不均衡样本\n",
    "model_svm = SVC(class_weight='balanced')                # 创建SVC模型对象并指定类别权重\n",
    "model_svm.fit(x, y)             # 输入x和y并训练模型\n",
    "\n",
    "\n",
    "# 使用集成方法EasyEnsemble处理不均衡样本\n",
    "#model_EasyEnsemble = EasyEnsemble()                     # 建立EasyEnsemble模型对象   ---old\n",
    "model_EasyEnsemble = EasyEnsembleClassifier()                     # 建立EasyEnsemble模型对象    ---new\n",
    "# x_EasyEnsemble_resampled, y_EasyEnsemble_resampled = model_EasyEnsemble.fit_sample(x, y)    # 输入数据并应用集成方法处理\n",
    "x_EasyEnsemble_resampled, y_EasyEnsemble_resampled = model_EasyEnsemble.fit(x, y) \n",
    "print (x_EasyEnsemble_resampled.shape)                  # 打印输出集成方法处理后的x样本集概况\n",
    "print (y_EasyEnsemble_resampled.shape)                  # 打印输出集成方法处理后的y标签集概况\n",
    "\n",
    "\n",
    "# 抽取其中一份数据做审查\n",
    "index_num = 1                   # 设置抽样样本集索引\n",
    "x_EasyEnsemble_resampled_t =pd.DataFrame(x_EasyEnsemble_resampled[index_num],columns=['col1','col2','col3','col4','col5'])      # 将数据转换为数据框并命名列名\n",
    "y_EasyEnsemble_resampled_t =pd.DataFrame(y_EasyEnsemble_resampled[index_num],columns=['label'])                     # 将数据转换为数据框并命名列名\n",
    "EasyEnsemble_resampled = pd.concat([x_EasyEnsemble_resampled_t,y_EasyEnsemble_resampled_t], axis = 1)               # 按列合并数据框\n",
    "groupby_data_EasyEnsemble =EasyEnsemble_resampled.groupby('label').count()                                          # 对label做分类汇总\n",
    "print (groupby_data_EasyEnsemble) # 打印输出经过EasyEnsemble处理后的数据集样本分类分布"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
